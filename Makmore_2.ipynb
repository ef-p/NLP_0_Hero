{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Our last method of taking the counts of each bigram won't scale at all. It will grow exponentionally. We're going to take an MLP approach"
      ],
      "metadata": {
        "id": "vCOZn94a6nfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to be following the approach of this 2003 paper: https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf.\n",
        "\n",
        "While this paper used a word level model, we'll be sticking to character level models, but with the same modeling approach.\n",
        "\n",
        "The paper took a vocabulary of 17,000 words and associated a 30 element feature vector to each, embedding them in a 30 dimensional space. The idea is that words that have similar meanings may end up near eachother in this 30D space, and conversly words that are very different, should be very far from each other. (Note: the paper mentions feature size numbers of 30, 60 and 100 in their experiments).\n",
        "\n",
        "The great power here is the ability to use closeness within the embedding space to generalize broadly.\n",
        "\n"
      ],
      "metadata": {
        "id": "o_sgAMBG7KUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "-PonkQP67JLU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read in all the words\n",
        "\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "words[:8]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vunbv2uIBe2k",
        "outputId": "88a8715e-d8e9-4f8e-c817-610a141ebfbf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LV8DpAr3Gkru",
        "outputId": "aff51677-fe11-466b-a11d-19a32b08f412"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32033"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the \"vocabulary\" of characters and the mapping to and from integers\n",
        "\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0 # string to int\n",
        "itos = {i:s for s, i in stoi.items()} # int to string\n",
        "print(stoi,'\\n',itos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCZcghYHGpTl",
        "outputId": "a5f23f15-131b-4bc0-84a0-6b4d969b94a3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0} \n",
            " {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here's how we build the dataset\n",
        "# this is the context length of how far back we're going to look to predict the next char.\n",
        "block_size = 3 # here we're looking at the last 3 characters to guess the 4th.\n",
        "\n",
        "X,Y = [], []\n",
        "# just the first five words for now .\n",
        "for w in words[:5]:\n",
        "  print(w)\n",
        "  context = [0]*block_size # creates a list the size of block_size,\n",
        "  for ch in w + '.':\n",
        "    ix = stoi[ch] #what's the indx of the character ?\n",
        "    X.append(context) # add the context, first list should be all '.'\n",
        "    Y.append(ix) # the index to get the character from itos.\n",
        "    print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
        "    #the above prints the current context, --> and then the next letter\n",
        "\n",
        "    context = context[1:] + [ix] # crop, and append the index for the next character.\n",
        "\n",
        "X,Y = torch.tensor(X), torch.tensor(Y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKt54n-aG04D",
        "outputId": "37760de5-bbcd-421d-872f-21d06485f54e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "emma\n",
            "... ---> e\n",
            "..e ---> m\n",
            ".em ---> m\n",
            "emm ---> a\n",
            "mma ---> .\n",
            "olivia\n",
            "... ---> o\n",
            "..o ---> l\n",
            ".ol ---> i\n",
            "oli ---> v\n",
            "liv ---> i\n",
            "ivi ---> a\n",
            "via ---> .\n",
            "ava\n",
            "... ---> a\n",
            "..a ---> v\n",
            ".av ---> a\n",
            "ava ---> .\n",
            "isabella\n",
            "... ---> i\n",
            "..i ---> s\n",
            ".is ---> a\n",
            "isa ---> b\n",
            "sab ---> e\n",
            "abe ---> l\n",
            "bel ---> l\n",
            "ell ---> a\n",
            "lla ---> .\n",
            "sophia\n",
            "... ---> s\n",
            "..s ---> o\n",
            ".so ---> p\n",
            "sop ---> h\n",
            "oph ---> i\n",
            "phi ---> a\n",
            "hia ---> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape, X.dtype, Y.shape, Y.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Yi40FlfJ-d5",
        "outputId": "b804fa53-d9db-48be-c3f7-80735cf21d0a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As you can see above we have 32 training examples that we made out of our first five names. Each of these examples is a triplet of charcters, made from our 27 possible characters. These are the triplets that appear when considering only the first five names, as we eventually use the entire dataset, we'll get more**"
      ],
      "metadata": {
        "id": "ZYB2mFiJN3Q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# here's our look up table.\n",
        "# the paper takes its 17000 words and puts them in spaces as small as 30\n",
        "# we'll start with a 2D space\n",
        "# 27 rows for our characters, and two columns for the size of the feature space\n",
        "C = torch.randn((27,2))"
      ],
      "metadata": {
        "id": "InazqVbZNtF9"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here's one embedding\n",
        "C[5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzgfVOnvPWz6",
        "outputId": "3d9ece71-8482-46e0-c616-44e57d984c51"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.1936, 0.5817])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#index using lists\n",
        "\n",
        "C[[5,6,7]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sP4WXy-3SX8M",
        "outputId": "c17a4d5f-2a43-4dc6-d46c-9cfb8d654315"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.1936,  0.5817],\n",
              "        [-0.0214, -1.8960],\n",
              "        [-0.4030, -0.1002]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can feed it a  tensor\n",
        "C[torch.tensor([5,6,7])]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3hAYR8ETUen",
        "outputId": "22c01d99-98ec-4b1b-b23f-8a7e6753d01a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.1936,  0.5817],\n",
              "        [-0.0214, -1.8960],\n",
              "        [-0.4030, -0.1002]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can even send the same index again and again\n",
        "C[torch.tensor([5,6,7,7,7,7,7,7])]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XankGhqTX3W",
        "outputId": "52c6c0ef-51a8-4832-b92b-e75920ef20ad"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.1936,  0.5817],\n",
              "        [-0.0214, -1.8960],\n",
              "        [-0.4030, -0.1002],\n",
              "        [-0.4030, -0.1002],\n",
              "        [-0.4030, -0.1002],\n",
              "        [-0.4030, -0.1002],\n",
              "        [-0.4030, -0.1002],\n",
              "        [-0.4030, -0.1002]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "C[X].shape # we can therefore embed all the ints in X with just C[X]\n",
        "# lets look at the shape to get this right in our heads."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3-lLIMcTb89",
        "outputId": "8c8f6562-3a73-410e-ec55-ce250082804e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we have 32 entries, each 3 characters long (our context length). and now each of the characters is identified by a 2D vector embedding. Let's just prove this to our selves before moving on."
      ],
      "metadata": {
        "id": "6xu0c77VV8G_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X[13,2] # lets pluck out a random int"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddcJN4KlTu_9",
        "outputId": "7ddc8849-5a21-442e-8a00-067efd5818c3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "C[X][13,2] # if we've done this correctly we should get the embedding for that int"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lw4a-GvVUOoM",
        "outputId": "4c05619e-e45b-4dd0-b48a-78c5ed1a7aa8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5771, 0.3914])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember, these number values are really characters. So, to reiterate, if we've got everything straight here the embedding for X[32,2] aka the embedding for tensor(1), should be [0.5771, 0.3914]. So if we've done this right we would expect C[X][13,2] == C[1], the previously mentioned embedding."
      ],
      "metadata": {
        "id": "tZPctZCKUjF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "C[1] #the value is the same! pytorch indexing is very handy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d3RDqzvUYmY",
        "outputId": "ac49b8e8-8f33-4208-ecb3-bee207f37f13"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5771, 0.3914])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " emb = C[X]"
      ],
      "metadata": {
        "id": "hGXPrhT_VlhS"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets make the hidden layer\n",
        "\n",
        "\n",
        "W1 = torch.randn((6, 100)) # we have 2D embeddings, and we have 3 of them, hence the 6. # the number of neurons is up to us.\n",
        "b1 = torch.randn(100)"
      ],
      "metadata": {
        "id": "TVetD2PrWcuO"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, ideally we'd like to do the following matmul: `emb @ W1 + b1`. But these matrix dims don't work. We'll have to fix this so our tensors are the same rank."
      ],
      "metadata": {
        "id": "qN8qNxJtW6I0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AP82YtCuXPgL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}